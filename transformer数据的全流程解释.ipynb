{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe0bce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utilities import Mytokenizer\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "import Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8357a97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ä¸­æ–‡å­—å…¸å­—æ•° 3643\n",
      "è‹±æ–‡å­—å…¸å­—æ•° 8349\n"
     ]
    }
   ],
   "source": [
    "train_path = './train.txt'\n",
    "data_path = './cmn.txt'\n",
    "tokenizer = Mytokenizer(data_path,'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d5f9c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mydataset(Dataset):\n",
    "    \"\"\"\n",
    "    @file_path æ•°æ®å­˜å‚¨ä½ç½®\n",
    "    @tokenizer å°†æ–‡å­—idåŒ–çš„å®ä¾‹åŒ–åçš„Tokenizer\n",
    "    @æ–‡ä»¶ä¸­ trgæ•°æ®çš„ä½ç½®\n",
    "    \n",
    "    ç”±äºDecoderçš„è¾“å…¥è¦æ±‚ï¼Œä¸€å¥è¯åº”è¢«åˆ‡åˆ†æˆå¤šæ®µï¼Œç›®æ ‡è¯æ•°æœ‰å¤šå°‘å°±åº”åˆ‡åˆ†å¤šå°‘æ¬¡\n",
    "    é‚£ä¹ˆç”¨ä¸€ä¸ªè‡ªå·±å†™åœ¨Transformerä¸­çš„batchç±»å°è£…ä¸€å¥è¯ï¼Œä¼šè‡ªåŠ¨çš„ç»™å¥å­maskï¼Œ\n",
    "    å¹¶ä¸”åˆ‡åˆ†Decoderçš„è¾“å…¥è¾“å‡º,ä½†æ˜¯æ³¨æ„ä¾‹å¦‚:\n",
    "    trgä¸º \"I love food\" é‚£ä¹ˆåº”è¯¥è¾“å…¥å››æ¬¡ï¼Œè¾“å‡ºå››æ¬¡\n",
    "        è¾“å…¥                   è¾“å‡º\n",
    "    <BOS> mask mask mask   I \n",
    "    <BOS> I    mask mask   I love\n",
    "    <BOS> I    love mask   I love food\n",
    "    <BOS> I    love food   I love food <EOS>\n",
    "    \n",
    "    æ–‡ä»¶ä¸­çš„æ ·å­å¦‚ä¸‹ï¼Œä¸­è¯‘è‹±ä»»åŠ¡è‹±æ–‡ä¸ºç›®æ ‡è¯­è¨€ trg_indexåº”ä¸º0\n",
    "    Hi\tå—¨\n",
    "    Hi\tä½  å¥½\n",
    "    Run\tä½  ç”¨ è·‘ çš„\n",
    "    \"\"\"\n",
    "    def __init__(self,file_path,tokenizer,trg_index=0):\n",
    "        self.tokenizer = tokenizer\n",
    "        #è¯»å–æ‰€æœ‰æ–‡æœ¬\n",
    "        with open(file_path,'r',encoding='utf8') as f:\n",
    "            self.lines = f.readlines()\n",
    "        #self.trg_count_words_line = []\n",
    "        self.length = len(self.lines)\n",
    "        self.trg_index = trg_index\n",
    "        if trg_index==0:\n",
    "            self.src_index = 1\n",
    "        else:\n",
    "            self.src_index = 0\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        src = self.lines[index].split('\\t')[self.src_index]\n",
    "        src = src.split('\\n')[0]\n",
    "        print(\"src:\",src)\n",
    "        trg = self.lines[index].split('\\t')[self.trg_index]\n",
    "        print(\"trg:\",trg)\n",
    "        #å¦‚ä¸Šé¢çš„ä¾‹å­ ä¸‰ä¸ªè¯çš„å¥å­åº”æœ‰å››ä¸ªæ ·æœ¬,æ‰€ä»¥åº”è¯¥æ‹·è´ä¸‰æ¬¡\n",
    "        copy_time = len(trg.split(' '))\n",
    "        print(copy_time)\n",
    "        # src_idåŒ– è¿™é‡Œç®€å•å®šä¹‰äº†srcä½¿ç”¨ä¸­æ–‡\n",
    "        src_id = self.tokenizer.ch_token_id([src],len(src.split(' ')))\n",
    "        print(\"src_id:\",src_id)\n",
    "        trg_id = self.tokenizer.en_token_id([trg],len(trg.split(' '))+2)\n",
    "        print(\"trg_id:\",trg_id)\n",
    "        src_tensor = torch.LongTensor(src_id)\n",
    "        trg_tensor = torch.LongTensor(trg_id)\n",
    "        print('src_tensor:',src_tensor.shape,'trg_tensor:',trg_tensor.shape)\n",
    "        #å¤åˆ¶   \n",
    "        #src_tensor = src_tensor.repeat(copy_time+1,1)\n",
    "        #trg_tensor = trg_tensor.repeat(copy_time+1,1)\n",
    "        #print(src_tensor)\n",
    "        #print(trg_tensor)\n",
    "        b = Transformer.Batch(src_tensor,trg_tensor)\n",
    "        print('æ•°æ®æœ€ç»ˆå½¢æ€')\n",
    "        print('è¾“å…¥',b.trg)\n",
    "        print('è¾“å‡º',b.trg_y)\n",
    "        print('mask',b.trg_mask)\n",
    "        return b\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6610eca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23635\n",
      "src: æˆ‘ çœŸ è ¢\n",
      "trg: I'm so stupid\n",
      "3\n",
      "src_id: [[16, 226, 309]]\n",
      "trg_id: [[1, 23, 175, 698, 2]]\n",
      "src_tensor: torch.Size([1, 3]) trg_tensor: torch.Size([1, 5])\n",
      "æ•°æ®æœ€ç»ˆå½¢æ€\n",
      "è¾“å…¥ tensor([[  1,  23, 175, 698]])\n",
      "è¾“å‡º tensor([[ 23, 175, 698,   2]])\n",
      "mask tensor([[[ True, False, False, False],\n",
      "         [ True,  True, False, False],\n",
      "         [ True,  True,  True, False],\n",
      "         [ True,  True,  True,  True]]])\n"
     ]
    }
   ],
   "source": [
    "dataset = Mydataset(train_path,tokenizer)\n",
    "print(len(dataset))\n",
    "b1 = dataset.__getitem__(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ce252e",
   "metadata": {},
   "source": [
    "å°†è¦æµ‹è¯•è¾“å…¥è¾“å‡ºçš„æ¨¡å— å®ä¾‹åŒ–æ­¥éª¤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2a72586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YBC\\jupyter\\Transformer.py:398: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  nn.init.xavier_uniform(p)\n"
     ]
    }
   ],
   "source": [
    "src_vocab,trg_vocab = tokenizer.get_vocab()\n",
    "d_model = 512\n",
    "#è¿™æ˜¯ä¸€ä¸ªç®€å•çš„Transformerç½‘ç»œï¼Œåªæœ‰ä¸€å±‚encoder decoder æ³¨ï¼šæ— ç”Ÿæˆå™¨\n",
    "model = Transformer.make_model(src_vocab,trg_vocab,1,d_model)\n",
    "en_embedding = Transformer.Embeddings(d_model,trg_vocab)\n",
    "ch_embedding = Transformer.Embeddings(d_model,src_vocab)\n",
    "multihead_attention = Transformer.MultiHeadedAttention(8,d_model)\n",
    "pe = Transformer.PositionalEncoding(d_model,0.1,max_len=20)\n",
    "generater = Transformer.Generator(d_model,trg_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec520b",
   "metadata": {},
   "source": [
    "## æ•°æ®æ¨¡æ‹Ÿ\n",
    "é€šè¿‡æœªè®­ç»ƒçš„ç½‘ç»œè¿è¡Œæ•°æ®è·å¾—å¯¹åº”çš„æ•°æ®å½¢çŠ¶\\\n",
    "é¡ºåºä¸ºEmbedding->PositionEncoding->MultiheadAttention\\\n",
    "Encoder-MultiHead è¾“å…¥æ¥æºä¸ºq:src_tensor k:src_tensor v:src_tensor\\\n",
    "Dncoder-MultiHead\\\n",
    "è¾“å…¥æ¥æºä¸º\\\n",
    "1&emsp;q:trg_tensor&emsp;k:trg_tensor&emsp;&emsp;&emsp;v:trg_tensor&emsp;mask:trg_mask\\\n",
    "2&emsp;q:trg_tensor&emsp;k:encoder_output&emsp;v:encoder_output&emsp;mask:src_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ca5ebf8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddingï¼š torch.Size([1, 3, 512]) torch.Size([1, 4, 512])\n",
      "PositionEncoding: torch.Size([1, 3, 512]) torch.Size([1, 4, 512])\n",
      "torch.Size([1, 1, 3])\n",
      "Encoder-Multihead: torch.Size([1, 3, 512])\n",
      "Decoder-masked-Multihead: torch.Size([1, 4, 512])\n",
      "è¿™é‡Œå¼€å§‹å®Œæ•´Transformerç»“æ„æ•°æ®æµç¨‹\n",
      "è¾“å…¥æ¨¡å‹çš„æ•°æ®ä¸º(å…¶å®å°±æ˜¯ä¸€å¥è¯å¤åˆ¶äº†å››é):\n",
      " tensor([[ 16, 226, 309],\n",
      "        [ 16, 226, 309],\n",
      "        [ 16, 226, 309],\n",
      "        [ 16, 226, 309]]) \n",
      " tensor([[  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698],\n",
      "        [  1,  23, 175, 698]])\n",
      "Total Model: torch.Size([4, 4, 512])\n",
      "Generater: torch.Size([4, 4, 8349])\n",
      "ç›®å‰æˆ‘ä»¬å·²ç»è·å¾—äº†å››å¥è¯ï¼Œå››ä¸ªè¯çš„idäº†,å½“ç„¶è¿™å››å¥åº”è¯¥æ˜¯\n",
      "I'm |<PAD>|<PAD>|<PAD>\n",
      "I'm |so | <PAD> |<PAD>\n",
      "I'm |so |stupid |<PAD>\n",
      "I'm |so |stupid |<EOS>\n",
      "é‚£ä¹ˆæˆ‘ä»¬éœ€è¦å°†è¿™å››å¥è¯çš„idåŒ–çš„å‘é‡ä¸generatorçš„è¾“å‡ºè®¡ç®—æŸå¤±\n",
      "ä½†è¿™é‡Œéœ€è¦æ³¨æ„ï¼Œæ¯æ¬¡åªè®¡ç®—ä¸€ä¸ªè¯çš„æŸå¤±ï¼Œå¦‚ç¬¬ä¸€è¡Œåº”è®¡ç®—I'm\n",
      "ç¬¬äºŒè¡Œåº”è®¡ç®—so ä»¥æ­¤ç±»æ¨ å…¶ä»–è¯çš„å·®è·ä¸è¿›è¡Œè®¡ç®—\n"
     ]
    }
   ],
   "source": [
    "#è¿™é‡Œæ˜¯æ¨¡æ‹Ÿä¸€ä¸ªæ ·æœ¬è¾“å…¥Transformerå†…éƒ¨æ•°æ®å¤„ç†æµç¨‹ï¼Œå¹¶ä¸”æ‰“å°ä»å„ä¸ªæ¨¡å—å‡ºæ¥çš„æ•°æ®å½¢çŠ¶\n",
    "src_tensor = ch_embedding(b1.src)\n",
    "trg_tensor = en_embedding(b1.trg)\n",
    "print(\"Embeddingï¼š\",src_tensor.shape,trg_tensor.shape)\n",
    "src_tensor = pe(src_tensor)\n",
    "trg_tensor = pe(trg_tensor)\n",
    "print(\"PositionEncoding:\",src_tensor.shape,trg_tensor.shape)\n",
    "print(b1.src_mask.shape)\n",
    "encoder_output = multihead_attention(src_tensor,src_tensor,src_tensor,b1.src_mask)\n",
    "print('Encoder-Multihead:',encoder_output.shape)\n",
    "decoder_output = multihead_attention(trg_tensor,trg_tensor,trg_tensor,b1.trg_mask)\n",
    "decoder_output = multihead_attention(trg_tensor,encoder_output,encoder_output,b1.src_mask)\n",
    "print('Decoder-masked-Multihead:',decoder_output.shape)\n",
    "print(\"è¿™é‡Œå¼€å§‹å®Œæ•´Transformerç»“æ„æ•°æ®æµç¨‹\")\n",
    "#è¿™é‡Œæ˜¯ç›´æ¥å°†æ•°æ®è¾“å…¥Transformer ä¸ä¸Šé¢ä»£ç æ— å…³\n",
    "trg_input = b1.trg\n",
    "copy_time = trg_input.shape[1]\n",
    "#print(trg_input.shape)\n",
    "trg_input = trg_input.repeat(copy_time,1)\n",
    "src_input = b1.src\n",
    "src_input = src_input.repeat(copy_time,1)\n",
    "print('è¾“å…¥æ¨¡å‹çš„æ•°æ®ä¸º(å…¶å®å°±æ˜¯ä¸€å¥è¯å¤åˆ¶äº†å››é):\\n',src_input,'\\n',trg_input)\n",
    "transformer_output = model(src_input,trg_input,b1.src_mask,b1.trg_mask)\n",
    "print(\"Total Model:\",transformer_output.shape)\n",
    "generater_output = generater(transformer_output)\n",
    "print(\"Generater:\",generater_output.shape)\n",
    "print(\"ç›®å‰æˆ‘ä»¬å·²ç»è·å¾—äº†å››å¥è¯ï¼Œå››ä¸ªè¯çš„idäº†,å½“ç„¶è¿™å››å¥åº”è¯¥æ˜¯\")\n",
    "print(\"I'm |<PAD>|<PAD>|<PAD>\")\n",
    "print(\"I'm |so | <PAD> |<PAD>\")\n",
    "print(\"I'm |so |stupid |<PAD>\")\n",
    "print(\"I'm |so |stupid |<EOS>\")\n",
    "print(\"é‚£ä¹ˆæˆ‘ä»¬éœ€è¦å°†è¿™å››å¥è¯çš„idåŒ–çš„å‘é‡ä¸generatorçš„è¾“å‡ºè®¡ç®—æŸå¤±\")\n",
    "print(\"ä½†è¿™é‡Œéœ€è¦æ³¨æ„ï¼Œæ¯æ¬¡åªè®¡ç®—ä¸€ä¸ªè¯çš„æŸå¤±ï¼Œå¦‚ç¬¬ä¸€è¡Œåº”è®¡ç®—I'm\\n\\\n",
    "ç¬¬äºŒè¡Œåº”è®¡ç®—so ä»¥æ­¤ç±»æ¨ å…¶ä»–è¯çš„å·®è·ä¸è¿›è¡Œè®¡ç®—\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2725661c",
   "metadata": {},
   "source": [
    "## mask å±•ç¤º\n",
    "ä¸‹é¢å±•ç¤ºä»¥ä¸‹maskåçš„decoderè¾“å…¥\n",
    "è¿™ä¸ªæ“ä½œå…¶å®æ˜¯åœ¨attentionä¸­æ‰§è¡Œçš„ï¼Œè¿™é‡Œåªæ˜¯ç®€å•å±•ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "90e4ce02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[          1, -1000000000, -1000000000, -1000000000],\n",
       "         [          1,          23, -1000000000, -1000000000],\n",
       "         [          1,          23,         175, -1000000000],\n",
       "         [          1,          23,         175,         698]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trg_input.masked_fill(b1.trg_mask==0,-1e9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a009a2f",
   "metadata": {},
   "source": [
    "## ä»¥ä¸Šä¸ºä¸€æ¡æ•°æ®å…¨éƒ¨æµç¨‹\n",
    "è¿™é‡Œæ²¡æœ‰æ¼”ç¤ºæŸå¤±å‡½æ•°è®¡ç®—ä»¥åŠä¼˜åŒ–è¿‡ç¨‹ï¼ŒæŸå¤±å‡½æ•°å“ˆä½›å­¦ä¹ ç‰ˆä½¿ç”¨äº†KLæ•£åº¦ï¼Œè€Œattentionè®ºæ–‡ä¸­ä½¿ç”¨äº†äº¤å‰ç†µï¼Œè¿™é‡Œä¸åšè¯„ä»·ï¼Œè¯·è‡ªè¡Œé€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°\\\n",
    "Transformerçš„ä¼˜åŒ–æ˜¯å¸¦æœ‰çƒ­èº«çš„ï¼Œ1ä¸ªå°æ—¶è¿çƒ­èº«éƒ½è·‘ä¸å®Œï¼Œæ‰€ä»¥å°±ç®—äº†\\\n",
    "ä¸€æ¡æ•°æ®çš„å¦‚ä½•å¤„ç†å¦‚ä½•èµ°è¿‡æ•´ä¸ªæ¨¡å‹çš„æ ·å­å·²ç»æ¼”ç¤ºäº†\\\n",
    "ç›¸ä¿¡ä½ è‚¯å®šä¼šæ‰¹é‡è®­ç»ƒäº†å§ğŸ¤¡\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
